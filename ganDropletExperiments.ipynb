{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ganDropletExperiments.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOVxRyz4P1AsdpTiQlhrRxf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-quilo/premiereDroplets/blob/main/ganDropletExperiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlrsETHXA54q"
      },
      "source": [
        "Generative adversarial Networks (GANs) for droplet experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R-_Qg8QAwl3"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import tensorflow.keras as tf\n",
        "import tensorflow\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras.backend as backend\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras import backend\n",
        "from keras.layers import Lambda\n",
        "from keras.constraints import Constraint\n",
        "from keras.initializers import RandomNormal\n",
        "\n",
        "from keras import optimizers\n",
        "from keras.utils import np_utils\n",
        "import tensorflow.keras as tf"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH94h3wnJS4i"
      },
      "source": [
        "The new class GANexperiments is defined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWmV1I4YJR6t"
      },
      "source": [
        "class ClipConstraint(Constraint):\n",
        "    # set clip value when initialized\n",
        "    def __init__(self, clip_value):\n",
        "        self.clip_value = clip_value\n",
        "\n",
        "    # clip model weights to hypercube\n",
        "    def __call__(self, weights):\n",
        "        return backend.clip(weights, -self.clip_value, self.clip_value)\n",
        "\n",
        "    # get the config\n",
        "    def get_config(self):\n",
        "        return {'clip_value': self.clip_value}\n",
        "\n",
        "# clip model weights to a given hypercube\n",
        "class dropletGAN():\n",
        "\n",
        "    def __init__(self, trainingData, latent_dim, GANorWGAN):\n",
        "\n",
        "        # Wasserstein loss\n",
        "        def wasserstein_loss(y_true, y_pred):\n",
        "            return backend.mean(y_true * y_pred)\n",
        "\n",
        "        self.trainingData = trainingData\n",
        "        self.nFeatures = self.trainingData.shape[1]\n",
        "        # Dimension of the latent space (noise)\n",
        "        self.latent_dim = latent_dim\n",
        "        self.constraint = 0.01\n",
        "        self.dropoutNumber = 0.5\n",
        "        self.alpha = 0.3\n",
        "        self.GANorWGAN = GANorWGAN\n",
        "        self.nameExperiment = '_dropletExperiments_'\n",
        "\n",
        "        self.c1_hist = []\n",
        "        self.c2_hist = []\n",
        "        self.g_hist = []\n",
        "\n",
        "        self.optimizer = tf.optimizers.Nadam()\n",
        "\n",
        "        if self.GANorWGAN == 'WGAN':\n",
        "            self.loss = wasserstein_loss\n",
        "        elif self.GANorWGAN == 'GAN':\n",
        "            self.loss = 'binary_crossentropy'\n",
        "\n",
        "        self.loss_gen = 'mse'\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "\n",
        "        # Build the encoder and decoder\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Only the generator is trained through the combined model, thus:\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Connecting models\n",
        "        noise_input = tf.Input(shape=self.latent_dim)\n",
        "        generator_output = self.generator(noise_input)\n",
        "        discriminator_output = self.discriminator(generator_output)\n",
        "\n",
        "        # The combined model stacks the autoencoder and discriminator\n",
        "        # The stacked model has one input and two outputs: the decoded input and the discriminator output\n",
        "        self.combined = tf.Model(noise_input, discriminator_output, name = 'dropletGAN')\n",
        "        self.combined.compile(loss=self.loss, loss_weights=[0.999, 0.001], optimizer=self.optimizer)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        init = RandomNormal(stddev=0.02)\n",
        "        const = ClipConstraint(0.01)\n",
        "\n",
        "        in_disc = tf.Input(shape=(self.nFeatures))\n",
        "        disc = tf.layers.LeakyReLU(self.alpha)(in_disc)\n",
        "        disc = tf.layers.BatchNormalization()(disc)\n",
        "        disc_output = tf.layers.Dense(1, activation='sigmoid')(disc)\n",
        "        discriminator = tf.Model(in_disc, disc_output, name='Discriminator')\n",
        "        discriminator.compile(loss=self.loss, optimizer=self.optimizer)\n",
        "\n",
        "        return discriminator\n",
        "\n",
        "    def build_generator(self):\n",
        "        #init = RandomNormal(stddev=0.02)\n",
        "        #init = tf.initializers.RandomNormal(stddev=0.02)\n",
        "\n",
        "        input_gen = tf.Input(shape=self.latent_dim)\n",
        "        gen = tf.layers.Dense(8)(input_gen)\n",
        "        gen = tf.layers.LeakyReLU(self.alpha)(gen)\n",
        "        gen = tf.layers.BatchNormalization()(gen)\n",
        "        gen = tf.layers.Dense(8)(gen)\n",
        "        gen = tf.layers.LeakyReLU(self.alpha)(gen)\n",
        "        gen = tf.layers.BatchNormalization()(gen)\n",
        "        gen_output = tf.layers.Dense(self.nFeatures)(gen)\n",
        "\n",
        "        generator = tf.Model(input_gen, gen_output, name='Generator')\n",
        "        generator.summary()\n",
        "        return generator\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50, n_critic=5):\n",
        "\n",
        "        # Load and pre process the data\n",
        "\n",
        "        X_all = self.trainingData\n",
        "\n",
        "        if self.GANorWGAN == 'WGAN':\n",
        "            real = -np.ones(batch_size)\n",
        "            fake = np.ones(batch_size)\n",
        "\n",
        "        if self.GANorWGAN == 'GAN':\n",
        "            real = np.ones(batch_size)\n",
        "            fake = np.zeros(batch_size)\n",
        "\n",
        "        # Training the model\n",
        "        for epoch in range(epochs):\n",
        "            c1_tmp, c2_tmp = list(), list()\n",
        "\n",
        "            # Training the discriminator more often than the generator\n",
        "            for _ in range(n_critic):\n",
        "                # Randomly selected samples and noise\n",
        "                randomIndex = np.random.randint(0, X_all.shape[0], size=batch_size)\n",
        "                noise = np.random.normal(0, 1, size=(batch_size, self.latent_dim))\n",
        "                # Select a random batch of input\n",
        "                real_seqs = X_all[randomIndex]\n",
        "\n",
        "                # Generate a batch of new outputs (in the latent space) predicted by the encoder\n",
        "                gen_seqs = self.generator.predict(noise)\n",
        "\n",
        "                # Train the discriminator\n",
        "                # The arbitrary noise is considered to be a \"real\" sample\n",
        "                d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n",
        "                c1_tmp.append(d_loss_real)\n",
        "                # The latent space generated by the encoder is considered a \"fake\" sample\n",
        "                d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n",
        "                c2_tmp.append(d_loss_fake)\n",
        "\n",
        "            self.c1_hist.append(np.mean(c1_tmp))\n",
        "            self.c2_hist.append(np.mean(c2_tmp))\n",
        "\n",
        "            # Training the stacked model\n",
        "            g_loss = self.combined.train_on_batch(noise, [gen_seqs, real])\n",
        "            self.g_hist.append(g_loss)\n",
        "            print(\"%d [C1 real: %f, C2 fake: %f], [G loss: %f]\" % (epoch, self.c1_hist[epoch], self.c2_hist[epoch], g_loss[0]))\n",
        "\n",
        "            # Checkpoint progress: Plot losses and predicted data\n",
        "            if epoch % sample_interval == 0:\n",
        "\n",
        "                self.plot_loss(epoch)\n",
        "                self.plot_values(epoch)\n",
        "                self.generator.save(self.directory_data + '/' + self.nameExperiment + GANorWGAN +\n",
        "                                            '_' + self.field_name + '_' + str(self.latent_dim) + '_' + str(epoch),\n",
        "                                            save_format='tf')\n",
        "\n",
        "                self.discriminator.save(self.directory_data + '/' + self.nameExperiment + GANorWGAN +\n",
        "                                        '_' + self.field_name + '_' + str(self.latent_dim) + '_' + str(epoch),\n",
        "                                        save_format='tf')\n",
        "\n",
        "    # Plots the (W)GAN related losses at every sample interval\n",
        "\n",
        "    def plot_loss(self, epoch):\n",
        "        fig = plt.figure()\n",
        "        plt.plot(self.c1_hist, c='red')\n",
        "        plt.plot(self.c2_hist, c='blue')\n",
        "        plt.plot(self.g_hist, c='green')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title(\"GAN Loss per Epoch\")\n",
        "        plt.legend(['C real', 'C fake', 'Generator'])\n",
        "\n",
        "        plt.savefig(self.directory_data + '/' + self.nameExperiment + GANorWGAN + '_' + self.field_name + '_' + '_' + str(epoch) +\n",
        "                    '_' + str(self.latent_dim) + '.png')\n",
        "        plt.close()\n",
        "\n",
        "    # Plots predicted in the first 8 latent dimension at every sample interval\n",
        "\n",
        "    def plot_values(self, epoch):\n",
        "        noise = np.random.normal(0, 1, size=(X_all.shape[0], self.latent_dim))\n",
        "        prediction = self.generator(noise)\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(X_all[:, i])\n",
        "        plt.title('Ground truth')\n",
        "        plt.imshow(prediction)\n",
        "        plt.title('Generated experiments')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.directory_data + '/' + self.nameExperiment + GANorWGAN + '_' + self.field_name + '_' + '_' + str(epoch) +\n",
        "                    '_' + str(self.latent_dim) + '.png')\n",
        "        plt.close()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7ER6dPZKHp5"
      },
      "source": [
        "Execute the class and train the GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "id": "pFNkzKZYKHNZ",
        "outputId": "296b3d09-38bd-49cf-c4e7-ac19931e66df"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    #Load data\n",
        "    directory_data = '/content/'\n",
        "    filename = 'normalisedDropletsExperiments.npy'\n",
        "    trainingData = np.load(directory_data + filename)\n",
        "    epochs = 10000\n",
        "    batch_size = 32\n",
        "    n_critic = 5\n",
        "    sample_interval = 1000\n",
        "    latent_dim = 4\n",
        "\n",
        "    #Training method\n",
        "    GANorWGAN = 'GAN'\n",
        "\n",
        "    dropGAN = dropletGAN(trainingData=trainingData,\n",
        "              latent_dim=latent_dim,\n",
        "              GANorWGAN=GANorWGAN)\n",
        "    advAE.train(epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              sample_interval=sample_interval,\n",
        "              n_critic = n_critic)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_43 (InputLayer)        [(None, 4)]               0         \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 8)                 40        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_52 (LeakyReLU)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_52 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_53 (LeakyReLU)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_53 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 9)                 81        \n",
            "=================================================================\n",
            "Total params: 257\n",
            "Trainable params: 225\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-094ff1b3d3b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m               \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m               n_critic = n_critic)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-1e711e34de62>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval, n_critic)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         pcs_trun = np.load(self.directory_data + '/' + 'pcs_' + self.field_name + '_' +\n\u001b[0;32m--> 138\u001b[0;31m                            self.observationPeriod + '.npy')\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '//pcs_normalisedDropletsExperiments.npy_data_150_to_1150.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L_SQJJyR-As"
      },
      "source": [
        "directory_data\n",
        "filename"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}